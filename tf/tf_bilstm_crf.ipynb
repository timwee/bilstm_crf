{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timwee/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data and embedding utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNK = \"$UNK$\"\n",
    "NUM = \"$NUM$\"\n",
    "NONE = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(fname):\n",
    "    result = {}\n",
    "    with open(fname, 'r') as f:\n",
    "        for idx, word in enumerate(f):\n",
    "            word = word.strip()\n",
    "            result[word] = idx\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_words = load_vocab(os.path.join(data_dir, \"words.txt\"))\n",
    "vocab_tags = load_vocab(os.path.join(data_dir, \"tags.txt\"))\n",
    "vocab_chars = load_vocab(os.path.join(data_dir, \"chars.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nwords = len(vocab_words)\n",
    "nchars = len(vocab_chars)\n",
    "ntags = len(vocab_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_filename = os.path.join(data_dir, \"glove.6B.300d.trimmed.npz\")\n",
    "with np.load(embeddings_filename) as data:\n",
    "    train_embeddings = data[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CoNLLDataset(object):\n",
    "    \"\"\"Class that iterates over CoNLL Dataset\n",
    "\n",
    "    __iter__ method yields a tuple (words, tags)\n",
    "        words: list of raw words\n",
    "        tags: list of raw tags\n",
    "\n",
    "    If processing_word and processing_tag are not None,\n",
    "    optional preprocessing is appplied\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        data = CoNLLDataset(filename)\n",
    "        for sentence, tags in data:\n",
    "            pass\n",
    "        ```\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, filename, processing_word=None, processing_tag=None,\n",
    "                 max_iter=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filename: path to the file\n",
    "            processing_words: (optional) function that takes a word as input\n",
    "            processing_tags: (optional) function that takes a tag as input\n",
    "            max_iter: (optional) max number of sentences to yield\n",
    "\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        self.processing_word = processing_word\n",
    "        self.processing_tag = processing_tag\n",
    "        self.max_iter = max_iter\n",
    "        self.length = None\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        niter = 0\n",
    "        with open(self.filename) as f:\n",
    "            words, tags = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if (len(line) == 0 or line.startswith(\"-DOCSTART-\")):\n",
    "                    if len(words) != 0:\n",
    "                        niter += 1\n",
    "                        if self.max_iter is not None and niter > self.max_iter:\n",
    "                            break\n",
    "                        yield words, tags\n",
    "                        words, tags = [], []\n",
    "                else:\n",
    "                    ls = line.split(' ')\n",
    "                    word, tag = ls[0],ls[-1]\n",
    "                    if self.processing_word is not None:\n",
    "                        word = self.processing_word(word)\n",
    "                    if self.processing_tag is not None:\n",
    "                        tag = self.processing_tag(tag)\n",
    "                    words += [word]\n",
    "                    tags += [tag]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Iterates once over the corpus to set and store length\"\"\"\n",
    "        if self.length is None:\n",
    "            self.length = 0\n",
    "            for _ in self:\n",
    "                self.length += 1\n",
    "\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessor_f(vocab_words=None, vocab_chars=None,\n",
    "                    lowercase=False, chars=False, allow_unk=True):\n",
    "    \"\"\"Return lambda function that transform a word (string) into list,\n",
    "    or tuple of (list, id) of int corresponding to the ids of the word and\n",
    "    its corresponding characters.\n",
    "\n",
    "    Args:\n",
    "        vocab: dict[word] = idx\n",
    "\n",
    "    Returns:\n",
    "        f(\"cat\") = ([12, 4, 32], 12345)\n",
    "                 = (list of char ids, word id)\n",
    "\n",
    "    \"\"\"\n",
    "    def f(word):\n",
    "        # 0. get chars of words\n",
    "        if vocab_chars is not None and chars == True:\n",
    "            char_ids = []\n",
    "            for char in word:\n",
    "                # ignore chars out of vocabulary\n",
    "                if char in vocab_chars:\n",
    "                    char_ids += [vocab_chars[char]]\n",
    "\n",
    "        # 1. preprocess word\n",
    "        if lowercase:\n",
    "            word = word.lower()\n",
    "        if word.isdigit():\n",
    "            word = NUM\n",
    "\n",
    "        # 2. get id of word\n",
    "        if vocab_words is not None:\n",
    "            if word in vocab_words:\n",
    "                word = vocab_words[word]\n",
    "            else:\n",
    "                if allow_unk:\n",
    "                    word = vocab_words[UNK]\n",
    "                else:\n",
    "                    raise Exception(\"Unknow key is not allowed. Check that \"\\\n",
    "                                    \"your vocab (tags?) is correct\")\n",
    "\n",
    "        # 3. return tuple char ids, word id\n",
    "        if vocab_chars is not None and chars == True:\n",
    "            return char_ids, word\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _pad_sequences(sequences, pad_tok, max_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "    \"\"\"\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\n",
    "        sequence_padded +=  [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, pad_tok, nlevels=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "        nlevels: \"depth\" of padding, for the case where we have characters ids\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "\n",
    "    \"\"\"\n",
    "    if nlevels == 1:\n",
    "        max_length = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, sequence_length = _pad_sequences(sequences,\n",
    "                                            pad_tok, max_length)\n",
    "\n",
    "    elif nlevels == 2:\n",
    "        max_length_word = max([max(map(lambda x: len(x), seq))\n",
    "                               for seq in sequences])\n",
    "        sequence_padded, sequence_length = [], []\n",
    "        for seq in sequences:\n",
    "            # all words are same length now\n",
    "            sp, sl = _pad_sequences(seq, pad_tok, max_length_word)\n",
    "            sequence_padded += [sp]\n",
    "            sequence_length += [sl]\n",
    "\n",
    "        max_length_sentence = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, _ = _pad_sequences(sequence_padded,\n",
    "                [pad_tok]*max_length_word, max_length_sentence)\n",
    "        sequence_length, _ = _pad_sequences(sequence_length, 0,\n",
    "                max_length_sentence)\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "\n",
    "    \"\"\"\n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type\n",
    "\n",
    "def get_chunks(seq, tags):\n",
    "    \"\"\"Given a sequence of tags, group entities and their position\n",
    "\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "\n",
    "    \"\"\"\n",
    "    default = tags[NONE]\n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    chunks = []\n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Progbar(object):\n",
    "    \"\"\"Progbar class copied from keras (https://github.com/fchollet/keras/)\n",
    "\n",
    "    Displays a progress bar.\n",
    "    Small edit : added strict arg to update\n",
    "    # Arguments\n",
    "        target: Total number of steps expected.\n",
    "        interval: Minimum visual progress update interval (in seconds).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, width=30, verbose=1):\n",
    "        self.width = width\n",
    "        self.target = target\n",
    "        self.sum_values = {}\n",
    "        self.unique_values = []\n",
    "        self.start = time.time()\n",
    "        self.total_width = 0\n",
    "        self.seen_so_far = 0\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def update(self, current, values=[], exact=[], strict=[]):\n",
    "        \"\"\"\n",
    "        Updates the progress bar.\n",
    "        # Arguments\n",
    "            current: Index of current step.\n",
    "            values: List of tuples (name, value_for_last_step).\n",
    "                The progress bar will display averages for these values.\n",
    "            exact: List of tuples (name, value_for_last_step).\n",
    "                The progress bar will display these values directly.\n",
    "        \"\"\"\n",
    "\n",
    "        for k, v in values:\n",
    "            if k not in self.sum_values:\n",
    "                self.sum_values[k] = [v * (current - self.seen_so_far),\n",
    "                                      current - self.seen_so_far]\n",
    "                self.unique_values.append(k)\n",
    "            else:\n",
    "                self.sum_values[k][0] += v * (current - self.seen_so_far)\n",
    "                self.sum_values[k][1] += (current - self.seen_so_far)\n",
    "        for k, v in exact:\n",
    "            if k not in self.sum_values:\n",
    "                self.unique_values.append(k)\n",
    "            self.sum_values[k] = [v, 1]\n",
    "\n",
    "        for k, v in strict:\n",
    "            if k not in self.sum_values:\n",
    "                self.unique_values.append(k)\n",
    "            self.sum_values[k] = v\n",
    "\n",
    "        self.seen_so_far = current\n",
    "\n",
    "        now = time.time()\n",
    "        if self.verbose == 1:\n",
    "            prev_total_width = self.total_width\n",
    "            sys.stdout.write(\"\\b\" * prev_total_width)\n",
    "            sys.stdout.write(\"\\r\")\n",
    "\n",
    "            numdigits = int(np.floor(np.log10(self.target))) + 1\n",
    "            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n",
    "            bar = barstr % (current, self.target)\n",
    "            prog = float(current)/self.target\n",
    "            prog_width = int(self.width*prog)\n",
    "            if prog_width > 0:\n",
    "                bar += ('='*(prog_width-1))\n",
    "                if current < self.target:\n",
    "                    bar += '>'\n",
    "                else:\n",
    "                    bar += '='\n",
    "            bar += ('.'*(self.width-prog_width))\n",
    "            bar += ']'\n",
    "            sys.stdout.write(bar)\n",
    "            self.total_width = len(bar)\n",
    "\n",
    "            if current:\n",
    "                time_per_unit = (now - self.start) / current\n",
    "            else:\n",
    "                time_per_unit = 0\n",
    "            eta = time_per_unit*(self.target - current)\n",
    "            info = ''\n",
    "            if current < self.target:\n",
    "                info += ' - ETA: %ds' % eta\n",
    "            else:\n",
    "                info += ' - %ds' % (now - self.start)\n",
    "            for k in self.unique_values:\n",
    "                if type(self.sum_values[k]) is list:\n",
    "                    info += ' - %s: %.4f' % (k,\n",
    "                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n",
    "                else:\n",
    "                    info += ' - %s: %s' % (k, self.sum_values[k])\n",
    "\n",
    "            self.total_width += len(info)\n",
    "            if prev_total_width > self.total_width:\n",
    "                info += ((prev_total_width-self.total_width) * \" \")\n",
    "\n",
    "            sys.stdout.write(info)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if current >= self.target:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "\n",
    "        if self.verbose == 2:\n",
    "            if current >= self.target:\n",
    "                info = '%ds' % (now - self.start)\n",
    "                for k in self.unique_values:\n",
    "                    info += ' - %s: %.4f' % (k,\n",
    "                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n",
    "                sys.stdout.write(info + \"\\n\")\n",
    "\n",
    "    def add(self, n, values=[]):\n",
    "        self.update(self.seen_so_far+n, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholders\n",
    "\n",
    "# shape = (batch size, max length of sentence in batch)\n",
    "word_ids = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                name=\"word_ids\")\n",
    "\n",
    "# shape = (batch size)\n",
    "sequence_lengths = tf.placeholder(tf.int32, shape=[None],\n",
    "                name=\"sequence_lengths\")\n",
    "\n",
    "# shape = (batch size, max length of sentence, max length of word)\n",
    "char_ids = tf.placeholder(tf.int32, shape=[None, None, None],\n",
    "                name=\"char_ids\")\n",
    "\n",
    "# shape = (batch_size, max_length of sentence)\n",
    "word_lengths_v = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                name=\"word_lengths\")\n",
    "\n",
    "# shape = (batch size, max length of sentence in batch)\n",
    "labels = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                name=\"labels\")\n",
    "\n",
    "# hyper parameters\n",
    "dropout = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                name=\"dropout\")\n",
    "lr_var = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                name=\"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dim_char = 100\n",
    "hidden_size_char = 100\n",
    "hidden_size_lstm = 300\n",
    "nepochs          = 15\n",
    "conf_dropout     = 0.5\n",
    "batch_size       = 20\n",
    "lr_method        = \"adam\"\n",
    "lr_val               = 0.001\n",
    "lr_decay         = 0.9\n",
    "clip             = -1 # if negative, no clipping\n",
    "nepoch_no_imprv_conf  = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embeddings\n",
    "\n",
    "- [LSTM Cell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell)\n",
    "- [Embedding lookup](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word embeddings\n",
    "\n",
    "with tf.variable_scope(\"words\"):\n",
    "    _word_embeddings = tf.Variable(\n",
    "            train_embeddings,\n",
    "            name=\"_word_embeddings\",\n",
    "            dtype=tf.float32,\n",
    "            trainable=False)\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup\n",
    "    word_embeddings = tf.nn.embedding_lookup(_word_embeddings,\n",
    "            word_ids, name=\"word_embeddings\")\n",
    "\n",
    "with tf.variable_scope(\"chars\"):\n",
    "    \n",
    "    # get char embeddings matrix\n",
    "    _char_embeddings = tf.get_variable(\n",
    "            name=\"_char_embeddings\",\n",
    "            dtype=tf.float32,\n",
    "            shape=[nchars, dim_char])\n",
    "    char_embeddings = tf.nn.embedding_lookup(_char_embeddings,\n",
    "            char_ids, name=\"char_embeddings\")\n",
    "\n",
    "    # put the time dimension on axis=1\n",
    "    s = tf.shape(char_embeddings)\n",
    "    char_embeddings = tf.reshape(char_embeddings,\n",
    "            shape=[s[0]*s[1], s[-2], dim_char])\n",
    "    word_lengths = tf.reshape(word_lengths_v, shape=[s[0]*s[1]])\n",
    "\n",
    "    # bi lstm on chars\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell\n",
    "    cell_fw = tf.contrib.rnn.LSTMCell(hidden_size_char,\n",
    "            state_is_tuple=True)\n",
    "    cell_bw = tf.contrib.rnn.LSTMCell(hidden_size_char,\n",
    "            state_is_tuple=True)\n",
    "    _output = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw, cell_bw, char_embeddings,\n",
    "            sequence_length=word_lengths, dtype=tf.float32)\n",
    "\n",
    "    # read and concat output\n",
    "    _, ((_, output_fw), (_, output_bw)) = _output\n",
    "    output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "\n",
    "    # shape = (batch size, max sentence length, char hidden size)\n",
    "    output = tf.reshape(output,\n",
    "            shape=[s[0], s[1], 2*hidden_size_char])\n",
    "    word_embeddings = tf.concat([word_embeddings, output], axis=-1)\n",
    "\n",
    "word_embeddings =  tf.nn.dropout(word_embeddings, conf_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Defines self.logits\n",
    "\n",
    "For each word in each sentence of the batch, it corresponds to a vector\n",
    "of scores, of dimension equal to the number of tags.\n",
    "\"\"\"\n",
    "with tf.variable_scope(\"bi-lstm\"):\n",
    "    cell_fw = tf.contrib.rnn.LSTMCell(hidden_size_lstm)\n",
    "    cell_bw = tf.contrib.rnn.LSTMCell(hidden_size_lstm)\n",
    "    (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw, cell_bw, word_embeddings,\n",
    "            sequence_length=sequence_lengths, dtype=tf.float32)\n",
    "    output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "    output = tf.nn.dropout(output, conf_dropout)\n",
    "\n",
    "with tf.variable_scope(\"proj\"):\n",
    "    W = tf.get_variable(\"W\", dtype=tf.float32,\n",
    "            shape=[2*hidden_size_lstm, ntags])\n",
    "\n",
    "    b = tf.get_variable(\"b\", shape=[ntags],\n",
    "            dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "    nsteps = tf.shape(output)[1]\n",
    "    output = tf.reshape(output, [-1, 2*hidden_size_lstm])\n",
    "    pred = tf.matmul(output, W) + b\n",
    "    logits = tf.reshape(pred, [-1, nsteps, ntags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# pred\n",
    "# no-op for CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(\n",
    "        logits, labels, sequence_lengths)\n",
    "#self.trans_params = trans_params # need to evaluate it for decoding\n",
    "loss = tf.reduce_mean(-log_likelihood)\n",
    "\n",
    "# for tensorboard\n",
    "tf.summary.scalar(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timwee/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Defines self.train_op that performs an update on a batch\n",
    "\n",
    "Args:\n",
    "    lr_method: (string) sgd method, for example \"adam\"\n",
    "    lr: (tf.placeholder) tf.float32, learning rate\n",
    "    loss: (tensor) tf.float32 loss to minimize\n",
    "    clip: (python float) clipping of gradient. If < 0, no clipping\n",
    "\n",
    "\"\"\"\n",
    "_lr_m = lr_method.lower() # lower to make sure\n",
    "\n",
    "train_op = None\n",
    "with tf.variable_scope(\"train_step\"):\n",
    "    if _lr_m == 'adam': # sgd method\n",
    "        optimizer = tf.train.AdamOptimizer(lr_var)\n",
    "    elif _lr_m == 'adagrad':\n",
    "        optimizer = tf.train.AdagradOptimizer(lr_var)\n",
    "    elif _lr_m == 'sgd':\n",
    "        optimizer = tf.train.GradientDescentOptimizer(lr_var)\n",
    "    elif _lr_m == 'rmsprop':\n",
    "        optimizer = tf.train.RMSPropOptimizer(lr_var)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown method {}\".format(_lr_m))\n",
    "\n",
    "    if clip > 0: # gradient clipping if clip is positive\n",
    "        grads, vs     = zip(*optimizer.compute_gradients(loss))\n",
    "        grads, gnorm  = tf.clip_by_global_norm(grads, clip)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, vs))\n",
    "    else:\n",
    "        train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatches(data, minibatch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: generator of (sentence, tags) tuples\n",
    "        minibatch_size: (int)\n",
    "\n",
    "    Yields:\n",
    "        list of tuples\n",
    "\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = [], []\n",
    "    for (x, y) in data:\n",
    "        if len(x_batch) == minibatch_size:\n",
    "            yield x_batch, y_batch\n",
    "            x_batch, y_batch = [], []\n",
    "\n",
    "        if type(x[0]) == tuple:\n",
    "            x = zip(*x)\n",
    "        x_batch += [x]\n",
    "        y_batch += [y]\n",
    "\n",
    "    if len(x_batch) != 0:\n",
    "        yield x_batch, y_batch\n",
    "\n",
    "############################################################     \n",
    "def get_feed_dict(words, labels_val=None, lr_val=None, dropout_val=None):\n",
    "    \"\"\"Given some data, pad it and build a feed dictionary\n",
    "\n",
    "    Args:\n",
    "        words: list of sentences. A sentence is a list of ids of a list of\n",
    "            words. A word is a list of ids\n",
    "        labels_val: list of ids\n",
    "        lr: (float) learning rate\n",
    "        dropout: (float) keep prob\n",
    "\n",
    "    Returns:\n",
    "        dict {placeholder: value}\n",
    "\n",
    "    \"\"\"\n",
    "    # perform padding of the given data\n",
    "    char_ids_feed, word_ids_feed = zip(*words)\n",
    "    word_ids_feed, sequence_lengths_feed = pad_sequences(word_ids_feed, 0)\n",
    "    char_ids_feed, word_lengths_feed = pad_sequences(char_ids_feed, pad_tok=0,\n",
    "        nlevels=2)\n",
    "    # keys are the tensorflow vars\n",
    "    feed = {\n",
    "            word_ids: word_ids_feed,\n",
    "            sequence_lengths: sequence_lengths_feed,\n",
    "            char_ids: char_ids_feed,\n",
    "            word_lengths_v: word_lengths_feed\n",
    "        }\n",
    "    if labels_val is not None:\n",
    "        labels_val, _ = pad_sequences(labels_val, 0)\n",
    "        feed[labels] = labels_val\n",
    "\n",
    "    if lr_val is not None:\n",
    "        feed[lr_var] = lr_val\n",
    "\n",
    "    if dropout is not None:\n",
    "        feed[dropout] = dropout_val\n",
    "\n",
    "    return feed, sequence_lengths_feed\n",
    "    \n",
    "############################################################\n",
    "\n",
    "def predict_batch(words):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        words: list of sentences\n",
    "\n",
    "    Returns:\n",
    "        labels_pred: list of labels for each sentence\n",
    "        sequence_length\n",
    "\n",
    "    \"\"\"\n",
    "    fd, sequence_lengths = get_feed_dict(words, dropout_val=1.0)\n",
    "    \n",
    "    # get tag scores and transition params of CRF\n",
    "    viterbi_sequences = []\n",
    "    logits_v, trans_params_v = sess.run(\n",
    "            [logits, trans_params], feed_dict=fd)\n",
    "    #print(logits_v)\n",
    "    # iterate over the sentences because no batching in vitervi_decode\n",
    "    for logit, sequence_length in zip(logits_v, sequence_lengths):\n",
    "        logit = logit[:sequence_length] # keep only the valid steps\n",
    "        viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                logit, trans_params_v)\n",
    "        viterbi_sequences += [viterbi_seq]\n",
    "\n",
    "    return viterbi_sequences, sequence_lengths\n",
    "\n",
    "\n",
    "    \n",
    "############################################################\n",
    "def run_evaluate(test):\n",
    "    \"\"\"Evaluates performance on test set\n",
    "\n",
    "    Args:\n",
    "        test: dataset that yields tuple of (sentences, tags)\n",
    "\n",
    "    Returns:\n",
    "        metrics: (dict) metrics[\"acc\"] = 98.4, ...\n",
    "\n",
    "    \"\"\"\n",
    "    accs = []\n",
    "    correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "    for words, labels in minibatches(test, batch_size):\n",
    "        labels_pred, sequence_lengths = predict_batch(words)\n",
    "\n",
    "        for lab, lab_pred, length in zip(labels, labels_pred,\n",
    "                                         sequence_lengths):\n",
    "            lab      = lab[:length]\n",
    "            lab_pred = lab_pred[:length]\n",
    "            accs    += [a==b for (a, b) in zip(lab, lab_pred)]\n",
    "\n",
    "            lab_chunks      = set(get_chunks(lab, vocab_tags))\n",
    "            lab_pred_chunks = set(get_chunks(lab_pred,\n",
    "                                            vocab_tags))\n",
    "\n",
    "            correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "            total_preds   += len(lab_pred_chunks)\n",
    "            total_correct += len(lab_chunks)\n",
    "\n",
    "    p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "    r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "    f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "    acc = np.mean(accs)\n",
    "\n",
    "    return {\"acc\": 100*acc, \"f1\": 100*f1}\n",
    "\n",
    "\n",
    "############################################################\n",
    "def run_epoch(train, dev, epoch, file_writer, merged):\n",
    "    \"\"\"Performs one complete pass over the train set and evaluate on dev\n",
    "\n",
    "    Args:\n",
    "        train: dataset that yields tuple of sentences, tags\n",
    "        dev: dataset\n",
    "        epoch: (int) index of the current epoch\n",
    "\n",
    "    Returns:\n",
    "        f1: (python float), score to select model on, higher is better\n",
    "\n",
    "    \"\"\"\n",
    "    # progbar stuff for logging\n",
    "    nbatches = (len(train) + batch_size - 1) // batch_size\n",
    "    prog = Progbar(target=nbatches)\n",
    "\n",
    "    # iterate over dataset\n",
    "    for i, (words, labels) in enumerate(minibatches(train, batch_size)):\n",
    "        fd, _ = get_feed_dict(words, labels, lr_val=lr_val,\n",
    "                dropout_val=conf_dropout)\n",
    "\n",
    "        _, train_loss, summary = sess.run(\n",
    "                [train_op, loss, merged], feed_dict=fd)\n",
    "\n",
    "        prog.update(i + 1, [(\"train loss\", train_loss)])\n",
    "\n",
    "        # tensorboard\n",
    "        if i % 10 == 0:\n",
    "            file_writer.add_summary(summary, epoch*nbatches + i)\n",
    "\n",
    "    metrics = run_evaluate(dev)\n",
    "    msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "            for k, v in metrics.items()])\n",
    "    print(msg)\n",
    "\n",
    "    return metrics[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_session(sess, saver, dir_model):\n",
    "    \"\"\"Saves session = weights\"\"\"\n",
    "    if not os.path.exists(dir_model):\n",
    "        os.makedirs(dir_model)\n",
    "    saver.save(sess, dir_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_preprocessor = preprocessor_f(vocab_words=vocab_words, vocab_chars=vocab_chars, lowercase=True, chars=True)\n",
    "tag_preprocessor = preprocessor_f(vocab_words=vocab_tags, lowercase=False, allow_unk=False)\n",
    "\n",
    "dev   = CoNLLDataset(os.path.join(data_dir, \"eng.testa\"), word_preprocessor,\n",
    "                     tag_preprocessor, None)\n",
    "train = CoNLLDataset(os.path.join(data_dir, \"eng.train\"), word_preprocessor,\n",
    "                     tag_preprocessor, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "nepoch_no_imprv = 0 # for early stopping\n",
    "\n",
    "# tensorboard\n",
    "merged      = tf.summary.merge_all()\n",
    "dir_output = \"./results/test/\"\n",
    "dir_model  = dir_output + \"model.weights/\"\n",
    "path_log   = dir_output + \"log.txt\"\n",
    "file_writer = tf.summary.FileWriter(dir_output, sess.graph)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Merge/MergeSummary:0\", shape=(), dtype=string)\n",
      "Tensor(\"lr:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(merged), print(lr_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 15\n",
      "703/703 [==============================] - 445s - train loss: 2.9020   \n",
      "acc 97.23 - f1 83.21\n",
      "- new best score!\n",
      "Epoch 2 out of 15\n",
      "150/703 [=====>........................] - ETA: 233s - train loss: 1.4334"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(nepochs):\n",
    "    print(\"Epoch {:} out of {:}\".format(epoch + 1,\n",
    "                nepochs))\n",
    "\n",
    "    score = run_epoch(train, dev, epoch, file_writer, merged)\n",
    "    lr_val *= lr_decay # decay learning rate\n",
    "\n",
    "    # early stopping and saving best parameters\n",
    "    if score >= best_score:\n",
    "        nepoch_no_imprv = 0\n",
    "        save_session(sess, saver, dir_model)\n",
    "        best_score = score\n",
    "        print(\"- new best score!\")\n",
    "    else:\n",
    "        nepoch_no_imprv += 1\n",
    "        if nepoch_no_imprv >= nepoch_no_imprv_conf:\n",
    "            print(\"- early stopping {} epochs without \"\\\n",
    "                    \"improvement\".format(nepoch_no_imprv))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
